# evaluate_ensemble.py
"""
å¯¹æ¯”è¯„æµ‹ï¼šåŸºçº¿(prompt1) vs é›†æˆç»“æœ
"""
import os
import json
import numpy as np
import matplotlib.pyplot as plt
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from tabulate import tabulate

# é…ç½®
BASE_DIR = "D:/groundingdino_work/GroundingDINO-main"
ANNO_PATH = os.path.join(BASE_DIR, "data/coco/annotations/instances_val2017.json")
OUTPUT_DIR = os.path.join(BASE_DIR, "results")

# ===== ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨é›†æˆç»“æœæ–‡ä»¶ =====
BASELINE_SEEN = os.path.join(OUTPUT_DIR, "coco_seen_400imgs_prompt1.json")
BASELINE_UNSEEN = os.path.join(OUTPUT_DIR, "coco_unseen_100imgs_prompt1.json")
ENSEMBLE_SEEN = os.path.join(OUTPUT_DIR, "coco_seen_400imgs_ensemble.json")  # æ”¹ä¸ºé›†æˆç»“æœ
ENSEMBLE_UNSEEN = os.path.join(OUTPUT_DIR, "coco_unseen_100imgs_ensemble.json")  # æ”¹ä¸ºé›†æˆç»“æœ

# ç±»åˆ«åˆ’åˆ†
SEEN_CLS_IDS = {
    1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,
    21,22,23,24,25,27,28,31,32,33,34,35,36,37,38,39,
    40,41,42,43,44,46,47,48,49,50,51,62,63,64,65,67,
    70,72,73,74,75,76,77,78,79,80,87,88,89,90
}

UNSEEN_CLS_IDS = {52,53,54,55,56,57,58,59,60,61,81,82,84,85,86}

# ç±»åˆ«åç§°ï¼ˆåŒä¸Šï¼Œçœç•¥ï¼‰

def evaluate_results(coco_gt, result_file, cat_ids, name):
    """è¯„ä¼°æ£€æµ‹ç»“æœ"""
    print(f"\nè¯„ä¼° {name}...")
    
    if not os.path.exists(result_file):
        print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
        return None
    
    with open(result_file, 'r') as f:
        results = json.load(f)
    
    print(f"  æ£€æµ‹æ¡†æ•°: {len(results)}")
    
    if len(results) == 0:
        print("  âš ï¸ æ— æ£€æµ‹æ¡†")
        return None
    
    try:
        coco_dt = coco_gt.loadRes(results)
        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
        coco_eval.params.catIds = list(cat_ids)
        coco_eval.params.imgIds = list(set([d['image_id'] for d in results]))
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        
        metrics = {
            'AP@[0.5:0.95]': coco_eval.stats[0],
            'AP@0.5': coco_eval.stats[1],
            'AP@0.75': coco_eval.stats[2],
            'AP_small': coco_eval.stats[3],
            'AP_medium': coco_eval.stats[4],
            'AP_large': coco_eval.stats[5],
            'AR@1': coco_eval.stats[6],
            'AR@10': coco_eval.stats[7],
            'AR@100': coco_eval.stats[8]
        }
        
        return metrics
    
    except Exception as e:
        print(f"  è¯„ä¼°å‡ºé”™: {e}")
        return None

def plot_comparison(baseline_metrics, ensemble_metrics, title):
    """ç»˜åˆ¶å¯¹æ¯”æŸ±çŠ¶å›¾"""
    metrics_names = ['AP@0.5', 'AP@[0.5:0.95]', 'AR@100']
    baseline_vals = [baseline_metrics.get(m, 0) for m in metrics_names]
    ensemble_vals = [ensemble_metrics.get(m, 0) for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline (prompt1)', color='#2E86AB')
    bars2 = ax.bar(x + width/2, ensemble_vals, width, label='Ensemble (3 prompts)', color='#A23B72')
    
    ax.set_title(f'Baseline vs Ensemble - {title}', fontsize=14, fontweight='bold')
    ax.set_xlabel('Evaluation Metrics', fontsize=12)
    ax.set_ylabel('Metric Value', fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_names)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    ax.set_ylim(0, max(max(baseline_vals), max(ensemble_vals)) * 1.2)
    
    for bar in bars1:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)
    
    for bar in bars2:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    return fig

def main():
    print("=" * 70)
    print("è¯„æµ‹ï¼šåŸºçº¿(prompt1) vs é›†æˆç»“æœ(3 prompts)")
    print("=" * 70)
    
    # åŠ è½½COCO GT
    print("\nåŠ è½½COCOæ ‡æ³¨...")
    coco_gt = COCO(ANNO_PATH)
    
    # è¯„ä¼°åŸºçº¿
    print("\n" + "=" * 50)
    print("åŸºçº¿ç»“æœè¯„ä¼° (prompt1)")
    print("=" * 50)
    
    baseline_seen = evaluate_results(coco_gt, BASELINE_SEEN, SEEN_CLS_IDS, "åŸºçº¿-SEEN")
    baseline_unseen = evaluate_results(coco_gt, BASELINE_UNSEEN, UNSEEN_CLS_IDS, "åŸºçº¿-UNSEEN")
    
    # è¯„ä¼°é›†æˆç»“æœ
    print("\n" + "=" * 50)
    print("é›†æˆç»“æœè¯„ä¼° (3 prompts)")
    print("=" * 50)
    
    ensemble_seen = evaluate_results(coco_gt, ENSEMBLE_SEEN, SEEN_CLS_IDS, "é›†æˆ-SEEN")
    ensemble_unseen = evaluate_results(coco_gt, ENSEMBLE_UNSEEN, UNSEEN_CLS_IDS, "é›†æˆ-UNSEEN")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "=" * 70)
    print("å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 70)
    
    if baseline_unseen and ensemble_unseen:
        # è¡¨æ ¼æ•°æ®
        table_data = [
            ['UNSEEN AP@0.5', 
             f"{baseline_unseen['AP@0.5']:.4f}", 
             f"{ensemble_unseen['AP@0.5']:.4f}",
             f"{ensemble_unseen['AP@0.5'] - baseline_unseen['AP@0.5']:+.4f}"],
            ['UNSEEN AP@[0.5:0.95]', 
             f"{baseline_unseen['AP@[0.5:0.95]']:.4f}", 
             f"{ensemble_unseen['AP@[0.5:0.95]']:.4f}",
             f"{ensemble_unseen['AP@[0.5:0.95]'] - baseline_unseen['AP@[0.5:0.95]']:+.4f}"],
            ['UNSEEN AR@100', 
             f"{baseline_unseen['AR@100']:.4f}", 
             f"{ensemble_unseen['AR@100']:.4f}",
             f"{ensemble_unseen['AR@100'] - baseline_unseen['AR@100']:+.4f}"]
        ]
        
        print("\nğŸ“Š UNSEENç±»åˆ«æ€§èƒ½å¯¹æ¯”:")
        print(tabulate(table_data, 
                      headers=['æŒ‡æ ‡', 'åŸºçº¿(prompt1)', 'é›†æˆ(3 prompts)', 'æå‡'],
                      tablefmt='grid'))
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        fig = plot_comparison(baseline_unseen, ensemble_unseen, 'UNSEEN Classes')
        fig.savefig(os.path.join(OUTPUT_DIR, 'comparison_ensemble.png'), dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"\nâœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {os.path.join(OUTPUT_DIR, 'comparison_ensemble.png')}")
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    report_path = os.path.join(OUTPUT_DIR, 'ensemble_comparison_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("æ–¹å‘Aï¼šå¤šæç¤ºé›†æˆ - åŸºçº¿ä¸é›†æˆç»“æœå¯¹æ¯”æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("æ”¹è¿›åŠ¨æœº:\n")
        f.write("  ä¸åŒpromptå¯èƒ½æ“…é•¿æ£€æµ‹ä¸åŒç±»åˆ«çš„ç›®æ ‡ï¼Œé›†æˆå¯ä»¥å–é•¿è¡¥çŸ­ã€‚\n\n")
        
        f.write("å®ç°ç»†èŠ‚:\n")
        f.write("  1. ä½¿ç”¨ä¸‰è½®promptå®éªŒçš„ç»“æœï¼šprompt1(çº¯ç±»å)ã€prompt2(æ¨¡æ¿å¥)ã€prompt3(ç»†ç²’åº¦æè¿°)\n")
        f.write("  2. å¯¹åŒä¸€ä¸ª(å›¾ç‰‡,ç±»åˆ«,ä½ç½®)çš„æ£€æµ‹æ¡†ï¼Œå–ä¸‰ä¸ªpromptä¸­çš„æœ€é«˜ç½®ä¿¡åº¦\n")
        f.write("  3. ä¿ç•™æ‰€æœ‰ä¸é‡å¤çš„æ£€æµ‹æ¡†\n\n")
        
        if baseline_unseen and ensemble_unseen:
            f.write("UNSEENç±»åˆ«æ€§èƒ½å¯¹æ¯”:\n")
            f.write("-" * 70 + "\n")
            f.write(f"{'æŒ‡æ ‡':<20} {'åŸºçº¿(prompt1)':<15} {'é›†æˆ(3 prompts)':<15} {'æå‡':<15}\n")
            f.write("-" * 70 + "\n")
            f.write(f"{'AP@0.5':<20} {baseline_unseen['AP@0.5']:<15.4f} "
                   f"{ensemble_unseen['AP@0.5']:<15.4f} "
                   f"{ensemble_unseen['AP@0.5'] - baseline_unseen['AP@0.5']:+.4f}\n")
            f.write(f"{'AP@[0.5:0.95]':<20} {baseline_unseen['AP@[0.5:0.95]']:<15.4f} "
                   f"{ensemble_unseen['AP@[0.5:0.95]']:<15.4f} "
                   f"{ensemble_unseen['AP@[0.5:0.95]'] - baseline_unseen['AP@[0.5:0.95]']:+.4f}\n")
            f.write(f"{'AR@100':<20} {baseline_unseen['AR@100']:<15.4f} "
                   f"{ensemble_unseen['AR@100']:<15.4f} "
                   f"{ensemble_unseen['AR@100'] - baseline_unseen['AR@100']:+.4f}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        if ensemble_unseen and baseline_unseen and ensemble_unseen['AP@0.5'] > baseline_unseen['AP@0.5']:
            f.write("âœ… ç»“è®ºï¼šå¤šæç¤ºé›†æˆç­–ç•¥æœ‰æ•ˆï¼Œæå‡äº†UNSEENç±»åˆ«æ£€æµ‹æ€§èƒ½\n")
        else:
            f.write("âš ï¸ ç»“è®ºï¼šå¤šæç¤ºé›†æˆç­–ç•¥æœªå¸¦æ¥æ˜æ˜¾æå‡ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æåŸå› \n")
    
    print(f"\nâœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

if __name__ == "__main__":
    main()