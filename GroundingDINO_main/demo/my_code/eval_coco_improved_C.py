# -*- coding: utf-8 -*-
"""
COCOæ£€æµ‹ç»“æœè¯„æµ‹è„šæœ¬ - å°å¹…æ”¹è¿›Cç‰ˆå¯¹æ¯”å®éªŒ
è®¡ç®—SEEN/UNSEENç±»COCOæŒ‡æ ‡ï¼Œå¹¶ä¸åŸºçº¿prompt1è¿›è¡Œå¯¹æ¯”
"""
import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# é…ç½®
BASE_DIR = "D:/groundingdino_work/GroundingDINO-main"
ANNO_PATH = os.path.join(BASE_DIR, "data/coco/annotations/instances_val2017.json")
OUTPUT_DIR = os.path.join(BASE_DIR, "results")

# Prompté…ç½®
BASELINE_PROMPT = "prompt1"  # åŸºçº¿ç‰ˆæœ¬
IMPROVED_PROMPT = "prompt1_improved_C"  # æ”¹è¿›ç‰ˆï¼ˆè‡ªé€‚åº”é˜ˆå€¼ï¼‰

PROMPT_NAME_MAP = {
    BASELINE_PROMPT: "Baseline (Uniform Threshold 0.25)",
    IMPROVED_PROMPT: "Improved C (Adaptive Threshold)"
}

# æ–‡ä»¶è·¯å¾„
BASELINE_SEEN_PATH = os.path.join(OUTPUT_DIR, f"coco_seen_400imgs_{BASELINE_PROMPT}.json")
BASELINE_UNSEEN_PATH = os.path.join(OUTPUT_DIR, f"coco_unseen_100imgs_{BASELINE_PROMPT}.json")
IMPROVED_SEEN_PATH = os.path.join(OUTPUT_DIR, f"coco_seen_400imgs_{IMPROVED_PROMPT}.json")
IMPROVED_UNSEEN_PATH = os.path.join(OUTPUT_DIR, f"coco_unseen_100imgs_{IMPROVED_PROMPT}.json")

# è¾“å‡ºæ–‡ä»¶
COMPARISON_REPORT_TXT = os.path.join(OUTPUT_DIR, "improvement_C_comparison_report.txt")
COMPARISON_BAR_PNG = os.path.join(OUTPUT_DIR, "improvement_C_comparison_bar.png")
COMPARISON_HEATMAP_PNG = os.path.join(OUTPUT_DIR, "improvement_C_comparison_heatmap.png")
IMPROVEMENT_METRICS_PNG = os.path.join(OUTPUT_DIR, "improvement_C_metrics.png")

# å•è½®å¯è§†åŒ–æ–‡ä»¶ï¼ˆç”¨äºæ”¹è¿›ç‰ˆè‡ªèº«ï¼‰
IMPROVED_METRICS_FULL_PNG = os.path.join(OUTPUT_DIR, "metrics_full_comparison_improved_C.png")
IMPROVED_SIZE_SENSITIVE_PNG = os.path.join(OUTPUT_DIR, "size_sensitive_metrics_improved_C.png")
IMPROVED_BBOX_ANALYSIS_PNG = os.path.join(OUTPUT_DIR, "bbox_analysis_improved_C.png")
IMPROVED_RECALL_CURVE_PNG = os.path.join(OUTPUT_DIR, "recall_curve_improved_C.png")

# SEEN: 65ä¸ªç±», UNSEEN: 15ä¸ªç±»
SEEN_CLS_IDS = {1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,27,28,31,32,33,34,35,36,37,38,39,40,41,42,43,44,46,47,48,49,50,51,62,63,64,65,67,70,72,73,74,75,76,77,78,79,80}
UNSEEN_CLS_IDS = {52,53,54,55,56,57,58,59,60,61,81,82,84,85,86}

UNSEEN_CLS_NAME = {
    52: "banana", 53: "apple", 54: "sandwich", 55: "orange", 56: "broccoli",
    57: "carrot", 58: "hot dog", 59: "pizza", 60: "donut", 61: "cake",
    81: "sink", 82: "refrigerator", 84: "book", 85: "clock", 86: "vase"
}

def load_detection_results(file_path):
    """åŠ è½½æ£€æµ‹ç»“æœæ–‡ä»¶"""
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        results = json.load(f)
    print(f"âœ… åŠ è½½æ£€æµ‹æ¡†ï¼š{file_path} | æ£€æµ‹æ¡†æ•°ï¼š{len(results)}")
    return results

def evaluate_coco_metrics(cocoGt, results, cat_ids, eval_name):
    """
    è®¡ç®—COCOå…¨é‡12é¡¹æŒ‡æ ‡
    å‚æ•°ï¼š
        cocoGt: COCO Ground Truth
        results: æ£€æµ‹æ¡†åˆ—è¡¨
        cat_ids: è¦è¯„ä¼°çš„ç±»åˆ«IDåˆ—è¡¨
        eval_name: è¯„æµ‹åç§°ï¼ˆSEEN/UNSEENï¼‰
    è¿”å›ï¼š
        metrics: 12é¡¹æŒ‡æ ‡å­—å…¸
    """
    print(f"\n===== å¼€å§‹{eval_name}ç±»åˆ«è¯„æµ‹ =====")
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹æ¡†ï¼Œç›´æ¥è¿”å›å…¨0æŒ‡æ ‡
    if len(results) == 0:
        print("âš ï¸ æ²¡æœ‰æ£€æµ‹æ¡†ï¼Œè¿”å›å…¨0æŒ‡æ ‡")
        metrics = {
            "AP@[0.5:0.95]": 0.0,
            "AP@0.5": 0.0,
            "AP@0.75": 0.0,
            "AP_small": 0.0,
            "AP_medium": 0.0,
            "AP_large": 0.0,
            "AR@1": 0.0,
            "AR@10": 0.0,
            "AR@100": 0.0,
            "AR_small": 0.0,
            "AR_medium": 0.0,
            "AR_large": 0.0
        }
        return metrics
    
    try:
        cocoDt = cocoGt.loadRes(results)
        cocoEval = COCOeval(cocoGt, cocoDt, "bbox")
        cocoEval.params.catIds = cat_ids
        cocoEval.params.imgIds = list(set([d['image_id'] for d in results]))
        cocoEval.evaluate()
        cocoEval.accumulate()
        cocoEval.summarize()
        
        if hasattr(cocoEval, 'stats') and len(cocoEval.stats) >= 12:
            metrics = {
                "AP@[0.5:0.95]": round(cocoEval.stats[0], 4),
                "AP@0.5": round(cocoEval.stats[1], 4),
                "AP@0.75": round(cocoEval.stats[2], 4),
                "AP_small": round(cocoEval.stats[3], 4),
                "AP_medium": round(cocoEval.stats[4], 4),
                "AP_large": round(cocoEval.stats[5], 4),
                "AR@1": round(cocoEval.stats[6], 4),
                "AR@10": round(cocoEval.stats[7], 4),
                "AR@100": round(cocoEval.stats[8], 4),
                "AR_small": round(cocoEval.stats[9], 4),
                "AR_medium": round(cocoEval.stats[10], 4),
                "AR_large": round(cocoEval.stats[11], 4)
            }
        else:
            print("âš ï¸ cocoEval.stats é•¿åº¦ä¸è¶³ï¼Œè¿”å›å…¨0æŒ‡æ ‡")
            metrics = {k: 0.0 for k in ["AP@[0.5:0.95]", "AP@0.5", "AP@0.75", "AP_small", 
                                        "AP_medium", "AP_large", "AR@1", "AR@10", "AR@100",
                                        "AR_small", "AR_medium", "AR_large"]}
    except Exception as e:
        print(f"âš ï¸ COCOè¯„æµ‹å‡ºé”™: {e}")
        metrics = {k: 0.0 for k in ["AP@[0.5:0.95]", "AP@0.5", "AP@0.75", "AP_small", 
                                    "AP_medium", "AP_large", "AR@1", "AR@10", "AR@100",
                                    "AR_small", "AR_medium", "AR_large"]}
    
    return metrics

def analyze_unseen_categories(unseen_results, cocoGt):
    """
    åˆ†æUNSEENç±»åˆ«å„æŒ‡æ ‡çš„è¯¦ç»†æƒ…å†µ
    è¿”å›ï¼š{cls_id: {"AP@[0.5:0.95]": value, "AP@0.5": value}}
    """
    if len(unseen_results) == 0:
        return {}
    
    try:
        cocoDt = cocoGt.loadRes(unseen_results)
        cocoEval = COCOeval(cocoGt, cocoDt, "bbox")
        cocoEval.params.catIds = list(UNSEEN_CLS_IDS)
        cocoEval.params.imgIds = list(set([d['image_id'] for d in unseen_results]))
        cocoEval.evaluate()
        cocoEval.accumulate()
        
        cls_metrics = {}
        precisions = cocoEval.eval['precision']
        
        for idx, cat_id in enumerate(cocoEval.params.catIds):
            if cat_id in UNSEEN_CLS_IDS:
                cat_precision = precisions[:, :, idx, 0, -1]
                if cat_precision.size > 0:
                    ap = np.mean(cat_precision[cat_precision > -1])
                    ap50 = np.mean(precisions[0, :, idx, 0, -1][precisions[0, :, idx, 0, -1] > -1])
                else:
                    ap, ap50 = 0.0, 0.0
                
                cls_metrics[cat_id] = {
                    "AP@[0.5:0.95]": round(ap, 4),
                    "AP@0.5": round(ap50, 4)
                }
        
        return cls_metrics
    except Exception as e:
        print(f"âš ï¸ åˆ†æUNSEENç±»åˆ«å‡ºé”™: {e}")
        return {}

def plot_improved_metrics(seen_metrics, unseen_metrics, title_suffix):
    """ç»˜åˆ¶æ”¹è¿›ç‰ˆçš„æŒ‡æ ‡å¯¹æ¯”å›¾"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    metrics_names = ["AP@[0.5:0.95]", "AP@0.5", "AP@0.75", "AR@100"]
    seen_vals = [seen_metrics[m] for m in metrics_names]
    unseen_vals = [unseen_metrics[m] for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(x - width/2, seen_vals, width, label="SEEN (65 classes)", color="#2E86AB")
    ax.bar(x + width/2, unseen_vals, width, label="UNSEEN (15 classes)", color="#A23B72")
    
    ax.set_title(f"COCO Core Metrics Comparison ({title_suffix})", fontsize=14, fontweight="bold")
    ax.set_xlabel("Metric Name", fontsize=12)
    ax.set_ylabel("Metric Value", fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_names)
    ax.legend(loc="upper right")
    ax.grid(axis="y", alpha=0.3)
    ax.set_ylim(0, 1.0)
    
    for i, v in enumerate(seen_vals):
        ax.text(i - width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    for i, v in enumerate(unseen_vals):
        ax.text(i + width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    
    plt.tight_layout()
    return fig

def plot_improved_size_sensitive(seen_metrics, unseen_metrics, title_suffix):
    """ç»˜åˆ¶æ”¹è¿›ç‰ˆçš„å°ºå¯¸æ•æ„ŸæŒ‡æ ‡å›¾"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    size_metrics = ["AP_small", "AP_medium", "AP_large"]
    seen_vals = [seen_metrics[m] for m in size_metrics]
    unseen_vals = [unseen_metrics[m] for m in size_metrics]
    
    x = np.arange(len(size_metrics))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(x - width/2, seen_vals, width, label="SEEN", color="#F18F01")
    ax.bar(x + width/2, unseen_vals, width, label="UNSEEN", color="#C73E1D")
    
    ax.set_title(f"Size-sensitive AP Metrics ({title_suffix})", fontsize=14, fontweight="bold")
    ax.set_xlabel("Object Size", fontsize=12)
    ax.set_ylabel("AP Value", fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(["Small", "Medium", "Large"])
    ax.legend(loc="upper right")
    ax.grid(axis="y", alpha=0.3)
    ax.set_ylim(0, 1.0)
    
    for i, v in enumerate(seen_vals):
        ax.text(i - width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    for i, v in enumerate(unseen_vals):
        ax.text(i + width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    
    plt.tight_layout()
    return fig

def plot_improved_bbox_analysis(seen_results, unseen_results, title_suffix):
    """ç»˜åˆ¶æ”¹è¿›ç‰ˆçš„æ£€æµ‹æ¡†åˆ†æå›¾"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    seen_scores = [det["score"] for det in seen_results]
    unseen_scores = [det["score"] for det in unseen_results]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    ax1.hist(seen_scores, bins=20, alpha=0.7, label="SEEN", color="#2E86AB", edgecolor="black")
    ax1.hist(unseen_scores, bins=20, alpha=0.7, label="UNSEEN", color="#A23B72", edgecolor="black")
    ax1.set_title(f"Detection Score Distribution ({title_suffix})", fontsize=12, fontweight="bold")
    ax1.set_xlabel("Detection Score", fontsize=10)
    ax1.set_ylabel("Count", fontsize=10)
    ax1.legend()
    ax1.grid(alpha=0.3)
    ax1.set_xlim(0.8, 1.0)
    
    unseen_cls_ids = [det["category_id"] for det in unseen_results 
                     if det["category_id"] in UNSEEN_CLS_NAME]
    unseen_cls_names = [UNSEEN_CLS_NAME[cid] for cid in unseen_cls_ids]
    
    cls_counts = {}
    for name in unseen_cls_names:
        cls_counts[name] = cls_counts.get(name, 0) + 1
    
    sorted_items = sorted(cls_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    cls_names = [item[0] for item in sorted_items]
    cls_vals = [item[1] for item in sorted_items]
    
    ax2.barh(cls_names, cls_vals, color="#F18F01", edgecolor="black")
    ax2.set_title("UNSEEN Category Detection Box Count (Top 10)", fontsize=12, fontweight="bold")
    ax2.set_xlabel("Detection Box Count", fontsize=10)
    ax2.set_ylabel("Category Name", fontsize=10)
    ax2.grid(axis="x", alpha=0.3)
    
    for i, v in enumerate(cls_vals):
        ax2.text(v + 1, i, f"{v}", va="center", fontsize=9, fontweight="bold")
    
    plt.tight_layout()
    return fig

def plot_improved_recall_curve(seen_metrics, unseen_metrics, title_suffix):
    """ç»˜åˆ¶æ”¹è¿›ç‰ˆçš„å¬å›ç‡æ›²çº¿"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    recall_metrics = ["AR@1", "AR@10", "AR@100"]
    seen_vals = [seen_metrics[m] for m in recall_metrics]
    unseen_vals = [unseen_metrics[m] for m in recall_metrics]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(recall_metrics, seen_vals, marker="o", linewidth=3, markersize=8, label="SEEN", color="#2E86AB")
    ax.plot(recall_metrics, unseen_vals, marker="s", linewidth=3, markersize=8, label="UNSEEN", color="#A23B72")
    
    ax_twin = ax.twinx()
    diffs = [s - u for s, u in zip(seen_vals, unseen_vals)]
    ax_twin.bar(recall_metrics, diffs, alpha=0.3, color="#F18F01", label="SEEN-UNSEEN Difference", edgecolor="black")
    
    ax.set_title(f"Recall Curve Comparison ({title_suffix})", fontsize=14, fontweight="bold")
    ax.set_xlabel("Recall Type", fontsize=12)
    ax.set_ylabel("AR Value", fontsize=12, color="#2E86AB")
    ax_twin.set_ylabel("Difference", fontsize=12, color="#F18F01")
    ax.legend(loc="upper left")
    ax_twin.legend(loc="upper right")
    ax.grid(alpha=0.3)
    ax.set_ylim(0, 1.0)
    
    for i, v in enumerate(seen_vals):
        ax.text(i, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    for i, v in enumerate(unseen_vals):
        ax.text(i, v - 0.02, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    
    plt.tight_layout()
    return fig

def plot_comparison_bar(baseline_metrics, improved_metrics):
    """ç»˜åˆ¶åŸºçº¿ä¸æ”¹è¿›ç‰ˆçš„å¯¹æ¯”æŸ±çŠ¶å›¾"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    # åªå¯¹æ¯”UNSEENç±»åˆ«çš„AP@0.5å’ŒAP@[0.5:0.95]
    metrics_names = ["AP@[0.5:0.95]", "AP@0.5"]
    baseline_vals = [baseline_metrics["unseen"][m] for m in metrics_names]
    improved_vals = [improved_metrics["unseen"][m] for m in metrics_names]
    
    x = np.arange(len(metrics_names))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(x - width/2, baseline_vals, width, label="Baseline (Uniform Threshold)", color="#2E86AB")
    ax.bar(x + width/2, improved_vals, width, label="Improved C (Adaptive Threshold)", color="#A23B72")
    
    ax.set_title("UNSEEN Metrics: Baseline vs Improved C", fontsize=14, fontweight="bold")
    ax.set_xlabel("Metric", fontsize=12)
    ax.set_ylabel("AP Value", fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_names)
    ax.legend(loc="upper right")
    ax.grid(axis="y", alpha=0.3)
    ax.set_ylim(0, 0.5)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(baseline_vals):
        ax.text(i - width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    for i, v in enumerate(improved_vals):
        ax.text(i + width/2, v + 0.01, f"{v:.3f}", ha="center", fontsize=10, fontweight="bold")
    
    plt.tight_layout()
    return fig

def plot_improvement_heatmap(baseline_cls_metrics, improved_cls_metrics):
    """ç»˜åˆ¶åŸºçº¿ä¸æ”¹è¿›ç‰ˆçš„ç±»åˆ«çƒ­åŠ›å›¾å¯¹æ¯”"""
    plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
    plt.rcParams["axes.unicode_minus"] = False
    
    # è·å–æ‰€æœ‰æœ‰æ•°æ®çš„UNSEENç±»åˆ«
    all_cls_ids = set()
    if baseline_cls_metrics:
        all_cls_ids.update(baseline_cls_metrics.keys())
    if improved_cls_metrics:
        all_cls_ids.update(improved_cls_metrics.keys())
    
    if not all_cls_ids:
        print("âš ï¸ æ²¡æœ‰UNSEENç±»åˆ«æ•°æ®ï¼Œè·³è¿‡çƒ­åŠ›å›¾")
        return None
    
    all_cls_ids = sorted(list(all_cls_ids))
    cls_names = [UNSEEN_CLS_NAME.get(cid, str(cid)) for cid in all_cls_ids]
    
    # æ„å»ºçƒ­åŠ›å›¾æ•°æ®
    heatmap_data = np.zeros((2, len(all_cls_ids)))
    versions = ["Baseline", "Improved C"]
    
    for j, cid in enumerate(all_cls_ids):
        if baseline_cls_metrics and cid in baseline_cls_metrics:
            heatmap_data[0, j] = baseline_cls_metrics[cid]["AP@0.5"]
        else:
            heatmap_data[0, j] = 0.0
            
        if improved_cls_metrics and cid in improved_cls_metrics:
            heatmap_data[1, j] = improved_cls_metrics[cid]["AP@0.5"]
        else:
            heatmap_data[1, j] = 0.0
    
    fig, ax = plt.subplots(figsize=(14, 4))
    sns.heatmap(heatmap_data, annot=True, fmt=".3f", cmap="YlGnBu",
                xticklabels=cls_names, yticklabels=versions,
                cbar_kws={"label": "AP@0.5"}, ax=ax)
    
    ax.set_title("UNSEEN Category AP@0.5: Baseline vs Improved C", fontsize=14, fontweight="bold")
    ax.set_xlabel("UNSEEN Category", fontsize=12)
    ax.set_ylabel("Version", fontsize=12)
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    return fig

def save_comparison_report(baseline_metrics, improved_metrics, 
                          baseline_cls_metrics, improved_cls_metrics):
    """ä¿å­˜å¯¹æ¯”æŠ¥å‘Š"""
    print("\nğŸ”„ ä¿å­˜å¯¹æ¯”æŠ¥å‘Š...")
    
    with open(COMPARISON_REPORT_TXT, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("å°å¹…æ”¹è¿›C - åŸºçº¿ä¸æ”¹è¿›ç‰ˆå¯¹æ¯”æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # æ•´ä½“æŒ‡æ ‡å¯¹æ¯”
        f.write("ğŸ“Š UNSEENç±»åˆ«æ•´ä½“æŒ‡æ ‡å¯¹æ¯”:\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'Metric':<20} {'Baseline':<15} {'Improved C':<15} {'Improvement':<15}\n")
        f.write("-" * 50 + "\n")
        
        metrics_to_compare = ["AP@[0.5:0.95]", "AP@0.5", "AP@0.75", "AR@100"]
        for metric in metrics_to_compare:
            baseline_val = baseline_metrics["unseen"][metric]
            improved_val = improved_metrics["unseen"][metric]
            improvement = improved_val - baseline_val
            f.write(f"{metric:<20} {baseline_val:<15.4f} {improved_val:<15.4f} {improvement:+.4f}\n")
        
        f.write("\n\nğŸ“Š SEENç±»åˆ«æ•´ä½“æŒ‡æ ‡å¯¹æ¯”:\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'Metric':<20} {'Baseline':<15} {'Improved C':<15} {'Improvement':<15}\n")
        f.write("-" * 50 + "\n")
        
        for metric in metrics_to_compare:
            baseline_val = baseline_metrics["seen"][metric]
            improved_val = improved_metrics["seen"][metric]
            improvement = improved_val - baseline_val
            f.write(f"{metric:<20} {baseline_val:<15.4f} {improved_val:<15.4f} {improvement:+.4f}\n")
        
        # å„UNSEENç±»åˆ«è¯¦ç»†å¯¹æ¯”
        f.write("\n\nğŸ“Š UNSEENå„ç±»åˆ«AP@0.5è¯¦ç»†å¯¹æ¯”:\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'Category':<15} {'Baseline':<15} {'Improved C':<15} {'Improvement':<15}\n")
        f.write("-" * 60 + "\n")
        
        all_cls_ids = set()
        if baseline_cls_metrics:
            all_cls_ids.update(baseline_cls_metrics.keys())
        if improved_cls_metrics:
            all_cls_ids.update(improved_cls_metrics.keys())
        
        for cls_id in sorted(all_cls_ids):
            cls_name = UNSEEN_CLS_NAME.get(cls_id, str(cls_id))
            baseline_val = baseline_cls_metrics.get(cls_id, {}).get("AP@0.5", 0.0) if baseline_cls_metrics else 0.0
            improved_val = improved_cls_metrics.get(cls_id, {}).get("AP@0.5", 0.0) if improved_cls_metrics else 0.0
            improvement = improved_val - baseline_val
            f.write(f"{cls_name:<15} {baseline_val:<15.4f} {improved_val:<15.4f} {improvement:+.4f}\n")
        
        # æ”¹è¿›æ€»ç»“
        f.write("\n\n" + "=" * 80 + "\n")
        f.write("æ”¹è¿›æ€»ç»“:\n")
        f.write("=" * 80 + "\n")
        
        unseen_ap_improve = improved_metrics["unseen"]["AP@0.5"] - baseline_metrics["unseen"]["AP@0.5"]
        f.write(f"â€¢ UNSEEN AP@0.5 æå‡: {unseen_ap_improve:+.4f}\n")
        
        if unseen_ap_improve > 0:
            f.write("âœ… è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥æœ‰æ•ˆï¼Œæå‡äº†UNSEENç±»åˆ«æ£€æµ‹æ€§èƒ½\n")
        else:
            f.write("âš ï¸ è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥æœªå¸¦æ¥æå‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\n")
    
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜ï¼š{COMPARISON_REPORT_TXT}")

def main():
    print("=" * 80)
    print("å°å¹…æ”¹è¿›C - åŸºçº¿ä¸æ”¹è¿›ç‰ˆå¯¹æ¯”è¯„æµ‹")
    print("=" * 80)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(BASELINE_SEEN_PATH):
        print(f"âŒ åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨ï¼š{BASELINE_SEEN_PATH}")
        return
    if not os.path.exists(IMPROVED_SEEN_PATH):
        print(f"âŒ æ”¹è¿›ç‰ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼š{IMPROVED_SEEN_PATH}")
        return
    
    try:
        # 1. åŠ è½½COCO Ground Truth
        print("\nğŸ”„ åŠ è½½COCO 2017 valæ ‡æ³¨æ–‡ä»¶...")
        cocoGt = COCO(ANNO_PATH)
        
        # 2. åŠ è½½åŸºçº¿ç»“æœ
        print("\nğŸ“Š åŠ è½½åŸºçº¿ç»“æœ...")
        baseline_seen = load_detection_results(BASELINE_SEEN_PATH)
        baseline_unseen = load_detection_results(BASELINE_UNSEEN_PATH)
        
        # 3. åŠ è½½æ”¹è¿›ç‰ˆç»“æœ
        print("\nğŸ“Š åŠ è½½æ”¹è¿›ç‰ˆç»“æœ...")
        improved_seen = load_detection_results(IMPROVED_SEEN_PATH)
        improved_unseen = load_detection_results(IMPROVED_UNSEEN_PATH)
        
        # 4. è®¡ç®—æŒ‡æ ‡
        print("\n" + "=" * 80)
        print("è¯„æµ‹åŸºçº¿ç»“æœ...")
        baseline_seen_metrics = evaluate_coco_metrics(cocoGt, baseline_seen, list(SEEN_CLS_IDS), "SEEN")
        baseline_unseen_metrics = evaluate_coco_metrics(cocoGt, baseline_unseen, list(UNSEEN_CLS_IDS), "UNSEEN")
        
        print("\n" + "=" * 80)
        print("è¯„æµ‹æ”¹è¿›ç‰ˆç»“æœ...")
        improved_seen_metrics = evaluate_coco_metrics(cocoGt, improved_seen, list(SEEN_CLS_IDS), "SEEN")
        improved_unseen_metrics = evaluate_coco_metrics(cocoGt, improved_unseen, list(UNSEEN_CLS_IDS), "UNSEEN")
        
        # 5. åˆ†æå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
        print("\n" + "=" * 80)
        print("åˆ†æUNSEENå„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡...")
        baseline_cls_metrics = analyze_unseen_categories(baseline_unseen, cocoGt)
        improved_cls_metrics = analyze_unseen_categories(improved_unseen, cocoGt)
        
        # 6. ç”Ÿæˆæ”¹è¿›ç‰ˆè‡ªèº«çš„å¯è§†åŒ–å›¾è¡¨
        print("\nğŸ”„ ç”Ÿæˆæ”¹è¿›ç‰ˆå¯è§†åŒ–å›¾è¡¨...")
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # æ”¹è¿›ç‰ˆå…¨é‡æŒ‡æ ‡å¯¹æ¯”å›¾
        fig = plot_improved_metrics(improved_seen_metrics, improved_unseen_metrics, "Improved C")
        fig.savefig(IMPROVED_METRICS_FULL_PNG, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print(f"âœ… æ”¹è¿›ç‰ˆå…¨é‡æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜ï¼š{IMPROVED_METRICS_FULL_PNG}")
        
        # æ”¹è¿›ç‰ˆå°ºå¯¸æ•æ„ŸæŒ‡æ ‡å›¾
        fig = plot_improved_size_sensitive(improved_seen_metrics, improved_unseen_metrics, "Improved C")
        fig.savefig(IMPROVED_SIZE_SENSITIVE_PNG, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print(f"âœ… æ”¹è¿›ç‰ˆå°ºå¯¸æ•æ„ŸæŒ‡æ ‡å›¾å·²ä¿å­˜ï¼š{IMPROVED_SIZE_SENSITIVE_PNG}")
        
        # æ”¹è¿›ç‰ˆæ£€æµ‹æ¡†åˆ†æå›¾
        fig = plot_improved_bbox_analysis(improved_seen, improved_unseen, "Improved C")
        fig.savefig(IMPROVED_BBOX_ANALYSIS_PNG, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print(f"âœ… æ”¹è¿›ç‰ˆæ£€æµ‹æ¡†åˆ†æå›¾å·²ä¿å­˜ï¼š{IMPROVED_BBOX_ANALYSIS_PNG}")
        
        # æ”¹è¿›ç‰ˆå¬å›ç‡æ›²çº¿
        fig = plot_improved_recall_curve(improved_seen_metrics, improved_unseen_metrics, "Improved C")
        fig.savefig(IMPROVED_RECALL_CURVE_PNG, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print(f"âœ… æ”¹è¿›ç‰ˆå¬å›ç‡æ›²çº¿å·²ä¿å­˜ï¼š{IMPROVED_RECALL_CURVE_PNG}")
        
        # 7. ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨
        print("\nğŸ”„ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
        
        # å¯¹æ¯”æŸ±çŠ¶å›¾
        baseline_metrics = {"seen": baseline_seen_metrics, "unseen": baseline_unseen_metrics}
        improved_metrics = {"seen": improved_seen_metrics, "unseen": improved_unseen_metrics}
        
        fig = plot_comparison_bar(baseline_metrics, improved_metrics)
        fig.savefig(COMPARISON_BAR_PNG, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print(f"âœ… å¯¹æ¯”æŸ±çŠ¶å›¾å·²ä¿å­˜ï¼š{COMPARISON_BAR_PNG}")
        
        # å¯¹æ¯”çƒ­åŠ›å›¾
        fig = plot_improvement_heatmap(baseline_cls_metrics, improved_cls_metrics)
        if fig:
            fig.savefig(COMPARISON_HEATMAP_PNG, dpi=300, bbox_inches="tight")
            plt.close(fig)
            print(f"âœ… å¯¹æ¯”çƒ­åŠ›å›¾å·²ä¿å­˜ï¼š{COMPARISON_HEATMAP_PNG}")
        
        # 8. ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        save_comparison_report(baseline_metrics, improved_metrics,
                              baseline_cls_metrics, improved_cls_metrics)
        
        # 9. æœ€ç»ˆæ±‡æ€»
        print("\n" + "=" * 80)
        print("è¯„æµ‹å®Œæˆï¼")
        print("=" * 80)
        print(f"\nåŸºçº¿ UNSEEN AP@0.5: {baseline_unseen_metrics['AP@0.5']:.4f}")
        print(f"æ”¹è¿›ç‰ˆ UNSEEN AP@0.5: {improved_unseen_metrics['AP@0.5']:.4f}")
        print(f"æå‡: {improved_unseen_metrics['AP@0.5'] - baseline_unseen_metrics['AP@0.5']:+.4f}")
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_DIR}")
        
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()