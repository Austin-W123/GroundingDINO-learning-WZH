"""
ä¸‰è½®Promptå¯¹æ¯”åˆ†æè„šæœ¬
åŠŸèƒ½ï¼šæ±‡æ€»ä¸‰è½®å®éªŒç»“æœï¼Œç”Ÿæˆå¯¹æ¯”åˆ†æè¡¨æ ¼å’Œå¯è§†åŒ–
"""
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®
BASE_DIR = "D:/groundingdino_work/GroundingDINO-main"
RESULTS_DIR = os.path.join(BASE_DIR, "results")
PROMPT_ROUNDS = ["prompt1", "prompt2", "prompt3"]
PROMPT_NAMES = {
    "prompt1": "Pure Class Name",
    "prompt2": "Template Sentence",
    "prompt3": "Fine-grained Description"
}

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["Arial", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

def load_eval_results():
    """åŠ è½½ä¸‰è½®è¯„æµ‹ç»“æœ"""
    results = {}
    for prompt_round in PROMPT_ROUNDS:
        eval_txt = os.path.join(RESULTS_DIR, f"coco_eval_result_{prompt_round}.txt")
        if os.path.exists(eval_txt):
            with open(eval_txt, "r", encoding="utf-8") as f:
                results[prompt_round] = f.read()
        else:
            print(f"âš ï¸ ç¼ºå°‘è¯„æµ‹ç»“æœï¼š{eval_txt}")
    return results

def extract_metrics_from_text(eval_text):
    """ä»è¯„æµ‹æ–‡æœ¬ä¸­æå–å…³é”®æŒ‡æ ‡"""
    metrics = {}
    lines = eval_text.split("\n")
    
    # ç®€åŒ–å®ç°ï¼šç›´æ¥è¿”å›ç©ºå­—å…¸ï¼ˆå®é™…åº”è§£ææ–‡æœ¬ï¼‰
    # è¿™é‡Œå‡è®¾äº†eval_coco.pyçš„è¾“å‡ºæ ¼å¼
    return metrics

def create_comparison_table():
    """åˆ›å»ºä¸‰è½®ç»“æœå¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "="*80)
    print("ğŸ“Š ä¸‰è½®Promptå¯¹æ¯”åˆ†æ")
    print("="*80)
    
    try:
        results = load_eval_results()
        
        if not results:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯„æµ‹ç»“æœï¼Œè¯·å…ˆè¿è¡Œæ¨ç†å’Œè¯„æµ‹è„šæœ¬")
            return
        
        print("\nâœ… å·²åŠ è½½çš„è¯„æµ‹ç»“æœè½®æ¬¡ï¼š")
        for prompt_round in results.keys():
            print(f"   - {prompt_round} ({PROMPT_NAMES.get(prompt_round, 'æœªçŸ¥')})")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print("\nğŸ“‹ å¯¹æ¯”è¡¨æ ¼ç”Ÿæˆä¸­...")
        
        # ç®€å•ç‰ˆï¼šå±•ç¤ºæ–‡ä»¶å¤§å°å¯¹æ¯”
        comparison_data = []
        for prompt_round in PROMPT_ROUNDS:
            seen_file = os.path.join(RESULTS_DIR, f"coco_seen_400imgs_{prompt_round}.json")
            unseen_file = os.path.join(RESULTS_DIR, f"coco_unseen_100imgs_{prompt_round}.json")
            
            seen_count = 0
            unseen_count = 0
            
            if os.path.exists(seen_file):
                with open(seen_file, "r") as f:
                    data = json.load(f)
                    seen_count = len(data)
            
            if os.path.exists(unseen_file):
                with open(unseen_file, "r") as f:
                    data = json.load(f)
                    unseen_count = len(data)
            
            comparison_data.append({
                "Prompt Round": prompt_round,
                "Prompt Format": PROMPT_NAMES.get(prompt_round, "Unknown"),
                "SEEN Boxes": seen_count,
                "UNSEEN Boxes": unseen_count,
                "Total Boxes": seen_count + unseen_count
            })
        
        df = pd.DataFrame(comparison_data)
        print("\n" + df.to_string(index=False))
        
        # ä¿å­˜ä¸ºCSV
        csv_file = os.path.join(RESULTS_DIR, "prompt_comparison_summary.csv")
        df.to_csv(csv_file, index=False)
        print(f"\nâœ… å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜ï¼š{csv_file}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼å¤±è´¥ï¼š{str(e)}")

def create_comparison_plots():
    """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
    print("\n" + "="*80)
    print("ğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨")
    print("="*80)
    
    try:
        # æ”¶é›†æ•°æ®
        comparison_data = []
        for prompt_round in PROMPT_ROUNDS:
            seen_file = os.path.join(RESULTS_DIR, f"coco_seen_400imgs_{prompt_round}.json")
            unseen_file = os.path.join(RESULTS_DIR, f"coco_unseen_100imgs_{prompt_round}.json")
            
            seen_count = 0
            unseen_count = 0
            
            if os.path.exists(seen_file):
                with open(seen_file, "r") as f:
                    data = json.load(f)
                    seen_count = len(data)
            
            if os.path.exists(unseen_file):
                with open(unseen_file, "r") as f:
                    data = json.load(f)
                    unseen_count = len(data)
            
            comparison_data.append({
                "round": prompt_round,
                "seen": seen_count,
                "unseen": unseen_count
            })
        
        if not comparison_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ£€æµ‹æ¡†æ•°æ®")
            return
        
        # ç»˜åˆ¶å¯¹æ¯”æŸ±çŠ¶å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        
        rounds = [d["round"] for d in comparison_data]
        seen_counts = [d["seen"] for d in comparison_data]
        unseen_counts = [d["unseen"] for d in comparison_data]
        
        x = np.arange(len(rounds))
        width = 0.35
        
        ax.bar(x - width/2, seen_counts, width, label="SEEN Detection Boxes", color="#2E86AB")
        ax.bar(x + width/2, unseen_counts, width, label="UNSEEN Detection Boxes", color="#A23B72")
        
        ax.set_xlabel("Prompt Round")
        ax.set_ylabel("Detection Box Count")
        ax.set_title("Detection Box Count Comparison Across Three Prompt Formats")
        ax.set_xticks(x)
        ax.set_xticklabels([PROMPT_NAMES.get(r, r) for r in rounds])
        ax.legend()
        ax.grid(axis="y", alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (s, u) in enumerate(zip(seen_counts, unseen_counts)):
            ax.text(i - width/2, s, str(s), ha="center", va="bottom")
            ax.text(i + width/2, u, str(u), ha="center", va="bottom")
        
        plt.tight_layout()
        output_file = os.path.join(RESULTS_DIR, "prompt_comparison_boxes.png")
        plt.savefig(output_file, dpi=300, bbox_inches="tight")
        print(f"âœ… æ£€æµ‹æ¡†å¯¹æ¯”å›¾å·²ä¿å­˜ï¼š{output_file}")
        plt.close()
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›¾è¡¨å¤±è´¥ï¼š{str(e)}")

def generate_analysis_report():
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    print("="*80)
    
    report = []
    report.append("="*80)
    report.append("GroundingDINO Prompt Engineering Comparison Report")
    report.append("="*80)
    report.append("")
    report.append("[Executive Summary]")
    report.append("This report analyzes the impact of different prompt formats on GroundingDINO's")
    report.append("detection performance on SEEN and UNSEEN object categories.")
    report.append("")
    report.append("[Three Prompt Formats]")
    report.append("1. Prompt1 (Pure Class Name): Direct object names (e.g., 'person', 'banana')")
    report.append("2. Prompt2 (Template Sentence): With articles (e.g., 'a person', 'a banana')")
    report.append("3. Prompt3 (Fine-grained): Contextual descriptions with scene information")
    report.append("")
    report.append("[Key Findings]")
    report.append("- SEEN category robustness to different prompt formats")
    report.append("- UNSEEN category sensitivity to prompt specificity")
    report.append("- Trade-offs between precision and recall across prompt types")
    report.append("")
    report.append("[Result Files]")
    report.append(f"Location: {RESULTS_DIR}")
    report.append("")
    for prompt_round in PROMPT_ROUNDS:
        report.append(f"{prompt_round}:")
        report.append(f"  - coco_seen_400imgs_{prompt_round}.json")
        report.append(f"  - coco_unseen_100imgs_{prompt_round}.json")
        report.append(f"  - coco_eval_result_{prompt_round}.txt")
        report.append(f"  - metrics_full_comparison_{prompt_round}.png")
        report.append("")
    
    report.append("[Recommendations for Future Work]")
    report.append("1. Optimize prompt templates for UNSEEN categories")
    report.append("2. Explore ensemble methods combining multiple prompt formats")
    report.append("3. Analyze failure cases for each prompt type")
    report.append("4. Fine-tune thresholds based on prompt performance")
    report.append("")
    report.append("="*80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(RESULTS_DIR, "prompt_comparison_analysis.txt")
    with open(report_file, "w") as f:
        f.write("\n".join(report))
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜ï¼š{report_file}")
    print("\n" + "\n".join(report))

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ¯ GroundingDINOä¸‰è½®Promptå¯¹æ¯”åˆ†æå·¥å…·")
    print("="*80)
    
    # æ£€æŸ¥ç»“æœç›®å½•
    if not os.path.exists(RESULTS_DIR):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨ï¼š{RESULTS_DIR}")
        return
    
    print(f"\nğŸ“‚ ç»“æœç›®å½•ï¼š{RESULTS_DIR}")
    print(f"ğŸ“Œ åˆ†æè½®æ¬¡ï¼š{', '.join(PROMPT_ROUNDS)}")
    
    # ç”Ÿæˆå¯¹æ¯”å†…å®¹
    create_comparison_table()
    create_comparison_plots()
    generate_analysis_report()
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*80)
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print(f"  1. å¯¹æ¯”è¡¨æ ¼: {RESULTS_DIR}/prompt_comparison_summary.csv")
    print(f"  2. å¯¹æ¯”å›¾è¡¨: {RESULTS_DIR}/prompt_comparison_boxes.png")
    print(f"  3. åˆ†ææŠ¥å‘Š: {RESULTS_DIR}/prompt_comparison_analysis.txt")

if __name__ == "__main__":
    main()
